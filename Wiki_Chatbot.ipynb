{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcV52WYJ4MIC7y3oudXtxt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2421bf66357e41079a30f66b0c0f875b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Machine learning",
              "Neural network (machine learning)",
              "Quantum machine learning",
              "Boosting (machine learning)",
              "Adversarial machine learning",
              "Attention (machine learning)",
              "Artificial intelligence",
              "Active learning (machine learning)",
              "Transformer (deep learning architecture)",
              "Support vector machine"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Select from available pages:",
            "description_tooltip": null,
            "disabled": false,
            "index": 5,
            "layout": "IPY_MODEL_3928e65642a1410faa8d5b0612cb2d2e",
            "style": "IPY_MODEL_d51fdb3f351e4a24b192618ceda324e2"
          }
        },
        "3928e65642a1410faa8d5b0612cb2d2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d51fdb3f351e4a24b192618ceda324e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achildress83/wiki-chatbot/blob/main/Wiki_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia --quiet\n",
        "!pip install openai==0.28 --quiet\n",
        "!pip install tiktoken --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63NVhxOXu63c",
        "outputId": "31373c8b-939a-439b-e822-3354602c5b2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia as wiki\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "import pandas as pd\n",
        "\n",
        "import openai\n",
        "from openai.embeddings_utils import distances_from_embeddings\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "eI93zw6xu9Tz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keys and constants\n",
        "openai.api_key = \"YOUR API KEY\"\n",
        "EMBEDDING_MODEL=\"text-embedding-ada-002\"\n",
        "COMPLETION_MODEL = \"gpt-3.5-turbo-instruct\"\n",
        "BATCH_SIZE=100\n",
        "MAX_CONTEXT_LENGHT = 4096\n",
        "MAX_TOKENS_OUTPUT=150"
      ],
      "metadata": {
        "id": "aY3mBkc7B2E_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "I used the wikipedia API and built a basic UI to allow the user to learn about any topic by selecting a topic and relevant page (from the subset of available pages under that topic). I wrote some basic preprocessing code. It's pretty robust, although it fails on certain pages for various reasons."
      ],
      "metadata": {
        "id": "3REN8SrsLxXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Functions for building chatbot\n",
        "\n",
        "def make_df(data):\n",
        "  \"\"\"\n",
        "  Takes wikipedia page data and returns a dataframe with the text column\n",
        "  after applying some basic cleaning.\n",
        "  \"\"\"\n",
        "  df = pd.DataFrame(data, columns=['text'])\n",
        "  # drop rows with no text, drop header rows, drop\n",
        "  df = df[(df['text'].str.len() > 0) & (~df['text'].str.contains('==')) & (df['text'].str.match('^[A-Za-z]'))]\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "  return df\n",
        "\n",
        "\n",
        "def make_embeddings_column(df, model=EMBEDDING_MODEL, batch_size=BATCH_SIZE):\n",
        "  \"\"\"\n",
        "  add embeddings column to dataframe limiting the batch size to not overload the API.\n",
        "  \"\"\"\n",
        "  embeddings = []\n",
        "\n",
        "  for i in range(0, len(df), batch_size):\n",
        "    response = openai.Embedding.create(input=df.iloc[i:i+batch_size]['text'].tolist(), model=model)\n",
        "    # add embeddings list\n",
        "    embeddings.extend([data.embedding for data in response.data])\n",
        "\n",
        "  # add embeddings list to df\n",
        "  df['embeddings'] = embeddings\n",
        "  return df\n",
        "\n",
        "\n",
        "def get_embedding(question, model):\n",
        "  \"\"\"\n",
        "  helper function for get_rows_sorted_by_relevance. Creates an embedding from the question.\n",
        "  \"\"\"\n",
        "  return openai.Embedding.create(input=question, model=model).data[0].embedding\n",
        "\n",
        "\n",
        "def get_rows_sorted_by_relevance(df, question, model=EMBEDDING_MODEL):\n",
        "    \"\"\"\n",
        "    Helper function for create_prompt. Takes in a question string and a dataframe containing\n",
        "    rows of text and associated embeddings, and returns that dataframe\n",
        "    sorted from least to most relevant for that question\n",
        "    \"\"\"\n",
        "\n",
        "    # Get embeddings for the question text\n",
        "    question_embeddings = get_embedding(question, model)\n",
        "\n",
        "    # Make a copy of the dataframe and add a \"distances\" column containing\n",
        "    # the cosine distances between each row's embeddings and the\n",
        "    # embeddings of the question\n",
        "    df_copy = df.copy()\n",
        "    df_copy[\"distances\"] = distances_from_embeddings(\n",
        "        question_embeddings,\n",
        "        df_copy[\"embeddings\"].values,\n",
        "        distance_metric=\"cosine\"\n",
        "    )\n",
        "\n",
        "    # Sort the copied dataframe by the distances and return it\n",
        "    # (shorter distance = more relevant so we sort in ascending order)\n",
        "    df_copy.sort_values(\"distances\", ascending=True, inplace=True)\n",
        "    return df_copy\n",
        "\n",
        "def create_prompt(df, question, max_token_count):\n",
        "    \"\"\"\n",
        "    Given a question and a dataframe containing rows of text and their\n",
        "    embeddings, return a text prompt to send to a Completion model\n",
        "    \"\"\"\n",
        "    # Create a tokenizer that is designed to align with our embeddings\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    # Count the number of tokens in the prompt template and question\n",
        "    prompt_template = \"\"\"\n",
        "    Answer the question based on the context below, and if the question\n",
        "    can't be answered based on the context, say \"I don't know\"\n",
        "\n",
        "    Context:\n",
        "\n",
        "    {}\n",
        "\n",
        "    ---\n",
        "\n",
        "    Question: {}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    current_token_count = len(tokenizer.encode(prompt_template)) + len(tokenizer.encode(question))\n",
        "\n",
        "    context = []\n",
        "\n",
        "    for text in get_rows_sorted_by_relevance(df, question)['text'].values:\n",
        "      # Add the length of the text to the current token count\n",
        "      current_token_count += len(tokenizer.encode(text))\n",
        "\n",
        "      # If current token count exceeds max, break\n",
        "      if current_token_count <= max_token_count:\n",
        "        context.append(text)\n",
        "\n",
        "      # Otherwise add the text to the context\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    return prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)\n",
        "\n",
        "def answer_question(\n",
        "    df, question, max_token_count=MAX_CONTEXT_LENGHT, max_answer_tokens=MAX_TOKENS_OUTPUT\n",
        "):\n",
        "    \"\"\"\n",
        "    Given a question, a dataframe containing rows of text, and a maximum\n",
        "    number of desired tokens in the prompt and response, return the\n",
        "    answer to the question according to an OpenAI Completion model\n",
        "\n",
        "    If the model produces an error, return an empty string\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = create_prompt(df, question, max_token_count)\n",
        "\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=COMPLETION_MODEL,\n",
        "            prompt=prompt,\n",
        "            max_tokens=max_answer_tokens\n",
        "        )\n",
        "        return response[\"choices\"][0][\"text\"].strip()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return \"\"\n",
        "\n",
        "def non_custom_answer(question):\n",
        "  answer = openai.Completion.create(\n",
        "    model=COMPLETION_MODEL,\n",
        "    prompt=QUESTION,\n",
        "    max_tokens=150\n",
        ")[\"choices\"][0][\"text\"].strip()\n",
        "  return answer"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qBvC6i1R1Uc8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot UI"
      ],
      "metadata": {
        "id": "VWldZ6q3vXNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title What topic do you want to learn about?\n",
        "topic = \"machine learning\" #@param {type:\"string\"}\n",
        "\n",
        "res = wiki.search(topic)\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=res,\n",
        "    description='Select from available pages:',\n",
        ")\n",
        "display(dropdown)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2421bf66357e41079a30f66b0c0f875b",
            "3928e65642a1410faa8d5b0612cb2d2e",
            "d51fdb3f351e4a24b192618ceda324e2"
          ]
        },
        "cellView": "form",
        "id": "gQaUUOpG4DjM",
        "outputId": "cd8caff7-e8b0-4233-b115-008db23717e8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Select from available pages:', options=('Machine learning', 'Neural network (machine lea…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2421bf66357e41079a30f66b0c0f875b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this cell everytime you change your wiki page\n",
        "# retrieve content from the selected wiki page\n",
        "page_name = dropdown.value\n",
        "data = wiki.page(page_name).content.split('\\n')\n",
        "# make a dataframe from the content\n",
        "df = make_df(data)\n",
        "# add an embeddings column to the df\n",
        "df = make_embeddings_column(df)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oIkMvdQpCbso"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title What's your question?\n",
        "QUESTION = \"What is the attention mechanism?\" #@param {type: \"string\"}\n",
        "\n",
        "display(Markdown(answer_question(df, question=QUESTION)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "cellView": "form",
        "id": "q-fjWRBwIDGW",
        "outputId": "b34fca1b-c26e-455f-ca99-e3c90fd2284c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The attention mechanism is a machine learning-based method that allows a model to focus on parts of an input sequence that are most relevant for the task at hand. It does this by calculating soft weights for each input element and using those weights to create a context vector that captures the important information. This method is used in natural language processing tasks like machine translation and has been extended within the Transformer architecture for more efficient processing."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer Comparison (w/ vs w/out prompt engineering)"
      ],
      "metadata": {
        "id": "TUKZVGreHU8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Question #1\n",
        "\n",
        "print(f'wiki page: {dropdown.value}')\n",
        "print(f'question: {QUESTION}')\n",
        "print('-----------------------------------------------------')\n",
        "print('non-custom response:')\n",
        "display(Markdown(non_custom_answer(question=QUESTION)))\n",
        "print('\\ncustom response:')\n",
        "display(Markdown(answer_question(df, question=QUESTION)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "cellView": "form",
        "id": "XgK_63L3EGq2",
        "outputId": "5ebd6f54-dd8b-4e99-ffe4-597221a0ddce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wiki page: Acquisition of Twitter by Elon Musk\n",
            "question: Who owns twitter?\n",
            "-----------------------------------------------------\n",
            "non-custom response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Twitter, Inc. is a publicly traded company and its ownership is distributed among its shareholders. It is not owned by any one individual or entity."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "custom response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Elon Musk."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Question #2\n",
        "\n",
        "print(f'wiki page: {dropdown.value}')\n",
        "print(f'question: {QUESTION}')\n",
        "print('-----------------------------------------------------')\n",
        "print('non-custom response:')\n",
        "display(Markdown(non_custom_answer(question=QUESTION)))\n",
        "print('\\ncustom response:')\n",
        "display(Markdown(answer_question(df, question=QUESTION)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "cellView": "form",
        "id": "avWpTrvhHHhu",
        "outputId": "561589c5-5c81-4411-caec-bd35a0f1fdef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wiki page: Gemini (language model)\n",
            "question: What is gemini?\n",
            "-----------------------------------------------------\n",
            "non-custom response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Gemini is the third astrological sign of the zodiac, represented by the symbol of the twins. People born between May 21st and June 20th are considered to be Geminis. They are known for their duality, adaptability, and communication skills. Geminis are often sociable and intellectual individuals who love to learn and express themselves. Mercury is the ruling planet of Gemini, and the element associated with this sign is air."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "custom response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Gemini is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. It was announced on December 6, 2023, positioned as a competitor to OpenAI's GPT-4. It powers the chatbot of the same name."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2htEjxGUIuhV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}